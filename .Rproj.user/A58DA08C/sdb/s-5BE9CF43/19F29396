{
    "collab_server" : "",
    "contents" : "---\ntitle: \"4.Decision Tree\"\nauthor: \"Affluence Tan\"\ndate: \"January 4, 2017\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n## 简单理解决策树\n\n###决策树的基本概念\n\n决策树顾名思义就像一颗树，一颗树有根，枝，叶。所以，决策树也有根决策节点，（枝）决策节点，叶节点。其中枝决策节点这名字是我自己取的。叶节点为什么不叫叶决策节点呢？因为到了叶节点就已经分出类了，不需要再做决策了。\n\n```{r eval=TRUE,echo=FALSE,message=FALSE,warning=FALSE}\nlibrary(party)\n```\n```{r eval=TRUE,echo=FALSE}\n# Create the input data frame.\ninput.dat <- readingSkills[c(1:105),]\n# Create the tree.\n  output.tree <- ctree(\n  nativeSpeaker ~ age + shoeSize + score, \n  data =  input.dat)\n# Plot the tree.\nplot(output.tree)\n```\n\n如上图就是一个决策树，一共有7个节点。其中节点1是根决策节点，节点2,3是（枝）决策节点，节点4,5,6,7是是叶节点。\n\n###选择最佳的分割\n\n建立决策树的第一个任务是要确定根据哪个特征（也就是自变量）就行分割。一般来说，选择能产生最大的信息增益的特征进行分割。下面解释什么是信息增益。\n\n####信息熵\n信息熵这个词是C．E．香农从热力学中借用过来的。热力学中的热熵是表示分子状态混乱程度的物理量。香农用信息熵的概念来描述信源的不确定度。公式为：\n\n$Entropy(S)=\\sum_{i=1}^c-p_ilog_2(p_i)$\n\n其中$S$代表数据的分割，比如一群信用卡卡主，有违约和不违约，$c=2$的分割。$p_i$为样本落入对应分割 $i$ 的比例（概率），如果违约的是50%，不违约当然也是50%。那么信息熵为：\n\n$Entropy(DefaultOrNot)=-0.5 \\times log_2(0.5) - 0.5 \\times log_2(0.5)=1$\n\n我们可以考察有两个类的分割的所有可能的熵，如下图。可以看到，熵总是在0到1之间，0代表信息纯度高，也就是信息内容多，意义明确。在本例中，如果一个人的违约率为100%或0%，那么也就意味着我们知道了这个人是否违约，很显然，比起50%的概率，信息的不确定性低了。\n\n```{r eval=TRUE,echo=FALSE}\ncurve(-x*log2(x)-(1-x)*log2(1-x),col=\"red\",xlab = \"Default rate\",ylab=\"Entropy\",lwd=4)\n```\n\n当按某个特征$F$分了$d$类以后，比如说，特征$F$为卡主的年薪，有$d=2$的特征类，年薪分别为>500K和<=500K，分别占样本数的20%和80%。违约率分别为40%和52.5%(注意违约率一定满足$0.525 \\times 0.8 + 0.2 \\times 0.4 = 0.5$)，那么对于$F_j$的熵为：\n\n$Entropy(F_j)=\\sum_{i=1}^c-p_{ij}log_2(p_{ij})$，其中：\n\n* $F_j$为第$j$个特征类，这里有两个，为年薪>500K和<=500K\n* $p_{ij}$为第$j$个特征类中样本落入的第$i$个分割的概率\n\n```{r caculate Entropy F}\n# Entropy(F1) card owner of annual salary >500k\nEntropy_f1 <- -0.4 * log2(0.4) - (1-0.4) * log2(1-0.4)\n# Entropy(F2) card owner of annual salary <=500k\nEntropy_f2 <- -0.525 * log2(0.525) - (1-0.525) * log2(1-0.525)\n```\n$Entropy(S_2)=\\sum_{j=1}^d{w_j}Entropy(F_j)$，其中：\n$w_j$为第$j$个特征类占总样本的比例\n```{r caculate Entropy S2}\n# Entropy(S1) based on feature F (annual salary)\nentropy_s2 <- 0.2 * Entropy_f1 + 0.8 * Entropy_f2\nentropy_s2\n```\n\n####信息增益\n\n$InfoGain(F)=Entropy(S)-Entropy(S_2) = 1 - 0.9927468 = 0.0072532$\n\n###修剪决策树\n\n一棵树可以无限制的增长下去，选择需要分割的特征，知道每个样本都归于一类，当然这样也就**过拟合**了。所以要修剪。\n\n####提前停止法\n\n一旦树达到了一定数量的决策，或者决策点只有少量案例，就停止增长。\n\n####后剪枝法\n\n如果树生长的过大，根据节点处的错误率修剪让树的大小合适。我们要用的C5.0就是用这种方法。\n\n## R实战\n\n### 获取数据集\n\n我将继续使用Naive Bayes用过的数据集audit，要预测的是income这个变量。\n```{r read dataset}\nlibrary(readr)\nadult <- read_csv(\"adult.csv\",na=\"?\")\nadult$income = as.factor(adult$income)\nstr(adult)\n```\n\n### 探索和准备数据集\n在学习NB分类的时候已经探索过了，通过画直方图我们看出了capital_gain这个变量对预测很重要。在这一章可以看到，决策树可以自动帮我们分析出这个结论。这个是它的优点。\n```{r explore and prapare data}\n# create training and test data\nadult_train <- adult[1:21000, ]\nadult_test <- adult[21001:32561, ]\n```\n\n### 基于数据集训练模型\n```{r message=FALSE,warning=FALSE}\n# load the \"C50\" library\nlibrary(C50)\nadult_classifier <- C5.0(income ~ ., data = adult_train)\nadult_classifier\nadult_predictions <- predict(adult_classifier, adult_test[,-15])\n```\n可以看到，有74个决策节点。\n\n### 评估模型性能\n```{r warning=FALSE}\n\n# load the \"gmodels\" library\nlibrary(gmodels)\n# Create the cross tabulation of predicted vs. actual\nCrossTable(x = adult_test$income, y = adult_predictions,\n           prop.chisq=FALSE)\n```\n模型的准确度为0.707+0.163 =0.87，如果你还记得NB分类的值，没错，0.791！，那是好上了太多。 而且在收入大于50K的人群中，有66.4%的人被挑选出来了。这是一个巨大的提升，我上次改进以后的模型也没有这么高。\n```{r warning=FALSE}\n#summary model\nsummary(adult_classifier)\n```\n最下面输出了Attribute usage，我上次分析出fnlwgt变量对model的预测没什么用，所以懒的类别化了，这里给出了结论。这个列表里面还有很多信息，可以慢慢琢磨。\n\n### 提高模型的性能\n\nC5.0算法对于C4.5有一个改进就是添加了一种自适应增加（adaptive boosting）算法。所谓的boosting就是把多种算法的优势组合起来。在C50函数中添加参数trials=10就可以了。\n```{r improve modeling}\nadult_classifier <- C5.0(income ~ ., data = adult_train, trials = 10)\nadult_classifier\nadult_predictions <- predict(adult_classifier, adult_test[,-15])\nCrossTable(x = adult_test$income, y = adult_predictions,\n           prop.chisq=FALSE)\n```\n额，结局有点打脸，模型性能反而降低了，主要是之前的性能已经很好了啦。\n\n## 小结\n\n决策树的优点很明显，比如它可以高度自动化的处理数值型和名义型变量，还有缺失值也可以。而且模型解释起来也比较容易，一下就能够看懂。\n缺点就不那么明显，比如容易过拟合或欠拟合，大特征的时候有偏等。\n\n总的来说，这是一个应用广泛的模型。\n\n[数据模型学习5--模型性能评估](Model_Performance.html)\n\n[系列文章首页](Preface.html)",
    "created" : 1484102348207.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2936247503",
    "id" : "19F29396",
    "lastKnownWriteTime" : 1484103431,
    "last_content_update" : 1484103431424,
    "path" : "D:/TanMiningWithPyR.github.io/Decision Tree.Rmd",
    "project_path" : "Decision Tree.Rmd",
    "properties" : {
    },
    "relative_order" : 9,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}