{
    "collab_server" : "",
    "contents" : "---\ntitle: \"从网页获取历史天气\"\nauthor: \"Affluence Tan\"\ndate: \"February 4, 2017\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n## 引言和动机\n\n在分析天池比赛数据集的时候，需要一些天气数据，所以只能到网上去爬。经过搜索，发现一个叫[天气后报](http://www.tianqihoubao.com)的网站，数据大而全，可以一用。尽管这是一个很简单的爬取过程（甚至称不上爬取），但还是有一些心得，可为后用。\n\n## 平台环境和使用的软件包\n\n我还是用R语言，过程中一个比较重要的领悟就是一定要用Linux平台或Mac平台进行，因为会涉及到中文编码的问题，并且这个问题如果用Windows在XML包中无法解决。\n```{r message=FALSE,warning=FALSE}\nlibrary(stringr)\nlibrary(XML)\nlibrary(readr)\nlibrary(plyr)\nlibrary(dplyr)\nlibrary(tidyr)\n```\n其中XML包是用来解析网页用的，别的都是一些数据处理的包。\n\n## 分析网站的结构\n\n这个网站结构简单，以上海2011年1月的天气数据为例，如下 http://www.tianqihoubao.com/lishi/shanghai/month/201101.html 。\n基本上就是www.tianqihoubao.com/lishi/ + 城市/month/ + 月份.html。\n\n所以生成一个向量，包含所有需要查询的城市和要查询的月份。\n\n```{r}\n# create a table , listing all citys and their English name\npinyin_city <- read_csv(\"pinyin_weather.csv\")\nhead(pinyin_city)\n\n# the common part of URLs\nroot_url <- \"http://www.tianqihoubao.com/lishi\"\n\n# the English names of city\ncity <- pinyin_city$en_name2\n\n# create the vector of dates\nmonth_str <- c(str_c(\"0\",1:9,sep = \"\"),\"10\",\"11\",\"12\")\nmonth <- c(str_c(\"2015\",month_str,\".html\",sep = \"\"),str_c(\"2016\",month_str,\".html\",sep = \"\"))\n\n# the function is for creating a vector of 24 months Url of one city\npaste_city_month <- function(city){\n  str_c(str_c(c(root_url,city,\"month/\"),collapse = \"/\"),month,sep = \"\")\n}\n\n# apply the function to all citys\ncity_permonth_url_list <- lapply(city,paste_city_month)\nnames(city_permonth_url_list) <- city\nhead(city_permonth_url_list,n = 3)\n```\n可以看到，最后生成了一个列表，包含了所有要爬取的网页。这里有个奇怪的地方，为什么要pinyin_city的数据集里有两个英文的城市名。因为天气后报这个网站有几个城市的英文名称有点怪异。导致爬取失败。由于是一次性的爬虫，我就没有利用异常处理，其实，在后面的程序中，如果使用了异常处理会更好。\n\n## 解析网页Parse\n\n我们利用XML包来解析网页。这个R包其实是调用了Libxml2，这是个C语言的XML库。在R包XML中，先用Libxml2解析网页为C的DOM树，再把它翻译成R语言的数据结构，变成R可以识别R的DOM树。\n```{r}\nweather_function <- function(city_permonth_url){\n  # Parse webpage,  with encoding gb2312\n  city_yearmonth <- htmlParse(city_permonth_url,encoding = \"gb2312\")\n  city_yearmonth_table <- readHTMLTable(city_yearmonth)[[1]] # 提取</table>中的内容\n  return(city_yearmonth_table)\n}\n```\n创建一个函数，对于每个URL，读取其中的一个表格。\n\n下面将函数应用到每一个URL上面，保存为csv文本。\n```{r, eval=FALSE}\nweather_df_list=list()\nfor (i in names(city_permonth_url_list)){\n  weather_df_list[[i]] <- lapply(city_permonth_url_list[[i]],weather_function)\n  city_weather <- bind_rows(weather_df_list[[i]])\n  file_name = str_c(i,\".csv\")\n  print(file_name)  # to show parsing process\n  write_csv(city_weather,file_name)\n}\n```\n这里每一个城市的天气会保存成一个文件，并且放在当前的工作目录下面。\n\n接下来清洗一下表格，把它改成有用的形式，有这么几步：\n\n* 把所有城市合成一张大表\n* 去掉换行符\n* 字符型转成数值型和日期型\n* 在大表中添上城市\n\n```{r, eval=FALSE}\n#  change work dir  \nsetwd(\"C:/Users/tanf/Desktop/tianchi/weather_city\")\n\n# bind all citys' table\ncity <- dir()\nnames(city) <- city\nread_citytable <- function(city_name){\n  weather_table <- read_csv(city_name)\n  weather_table[[\"city\"]] <- city_name\n  return(weather_table)\n}\n\ncity_weather_collection <- lapply(city,read_citytable)\ncity_weather <- bind_rows(city_weather_collection)\nnames(city_weather) <- c(\"date\",\"status\",\"temp\",\"wind_direction_force\",\"city_file_name\")\n# convert string to number\na <- city_weather %>% separate(col = status,into = c(\"first_status\",\"second_status\"),sep = \"/\") %>%\n    separate(col = temp,into = c(\"highest_temp\",\"lowest_temp\"),sep = \"/\") %>%\n    separate(col = wind_direction_force,into = c(\"first_wind\",\"second_wind\"),sep = \"/\") %>%\n    separate(col = first_wind,into = c(\"first_wind_direction\",\"first_wind_force\"),sep = \" \") %>%\n    separate(col = second_wind,into = c(\"second_wind_direction\",\"second_wind_force\"),sep = \" \")\na$date <- as.Date(a$date,\"%Y年%m月%d日\")\na$highest_temp <- as.integer(str_trim(str_replace(a$highest_temp,\"℃\\r\\n\",\"\")))\na$lowest_temp <- as.integer(str_trim(str_replace(a$lowest_temp,\"℃\",\"\")))\na$first_status <- str_trim(str_replace(a$first_status,\"\\r\\n\",\"\"))\na$first_wind_force <- str_trim(str_replace(a$first_wind_force,\"\\r\\n\",\"\"))\na$first_wind_direction <- str_trim(str_replace(a$first_wind_direction,\"\\r\\r\\n\",\"\"))\na <- a %>% separate(col = city_file_name,into = c(\"city\",\"csv\"),sep=\".csv\") %>% select(-contains(\"csv\"))\na$average_temp <- (a$highest_temp + a$lowest_temp)/2   \na <- left_join(x = a,y = pinyin_weather,by=c(\"city\" = \"en_name2\"))\na <- a[,c(-13,-10)]\nnames(a)[11] <- \"city_cn\"\n\n# output file named as \"city_weather_tanf.csv\"\" \nsetwd(\"C:/Users/tanf/Desktop/tianchi/dataset\")\nwrite_csv(a,\"city_weathter_tanf.csv\")\n```\n\n## 小结\n\n这只是一个很简单的网页数据采集，有很多的内容都没有涉及，比如利用XPATH语言查询（这个是网络数据采集的重头戏），还有利用Rcurl包和HTTP协议动态模拟浏览器登陆网站获取网页等技术。\n\n[返回该系列文章目录页](Preface_application.html)\n",
    "created" : 1486201164988.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1068652919",
    "id" : "DD09258C",
    "lastKnownWriteTime" : 1486370413,
    "last_content_update" : 1486370413881,
    "path" : "D:/TanMiningWithPyR.github.io/weather_scrapy.Rmd",
    "project_path" : "weather_scrapy.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}